{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the original model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
    "from keras.layers import Input\n",
    "from keras.utils.data_utils import get_file\n",
    "#from keras.regularizers import WeightRegularizer\n",
    "from keras.optimizers import Nadam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, History\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "#from theano.ifelse import ifelse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#name = 'bpi_12_w'\n",
    "name = 'helpdesk'\n",
    "args = {\n",
    "    'inputdir': '../input/{}/'.format(name),   \n",
    "    'outputdir': './output_files/{}/'.format(name)\n",
    "}\n",
    "\n",
    "args = argparse.Namespace(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(args.outputdir):\n",
    "    os.makedirs(args.outputdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(args.inputdir + 'parameters.pkl', \"rb\") as f:\n",
    "    maxlen = pickle.load(f)\n",
    "    num_features = pickle.load(f)\n",
    "    chartoindice = pickle.load(f)\n",
    "    targetchartoindice = pickle.load(f)\n",
    "    divisor = pickle.load(f)\n",
    "    divisor2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(args.inputdir + 'preprocessed_data.pkl', \"rb\") as f:\n",
    "    X = pickle.load(f)\n",
    "    y_a = pickle.load(f)\n",
    "    y_t = pickle.load(f)\n",
    "    X_test = pickle.load(f)\n",
    "    y_a_test = pickle.load(f)\n",
    "    y_t_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train on 7344 samples, validate on 1837 samples\n",
      "Epoch 1/500\n",
      "18s - loss: 2.1669 - act_output_loss: 0.9875 - time_output_loss: 1.1794 - val_loss: 1.7210 - val_act_output_loss: 0.6814 - val_time_output_loss: 1.0396\n",
      "Epoch 2/500\n",
      "23s - loss: 1.7598 - act_output_loss: 0.7207 - time_output_loss: 1.0391 - val_loss: 1.5862 - val_act_output_loss: 0.5849 - val_time_output_loss: 1.0014\n",
      "Epoch 3/500\n",
      "23s - loss: 1.7179 - act_output_loss: 0.6972 - time_output_loss: 1.0207 - val_loss: 1.5769 - val_act_output_loss: 0.5928 - val_time_output_loss: 0.9841\n",
      "Epoch 4/500\n",
      "21s - loss: 1.6845 - act_output_loss: 0.6762 - time_output_loss: 1.0083 - val_loss: 1.5819 - val_act_output_loss: 0.5964 - val_time_output_loss: 0.9855\n",
      "Epoch 5/500\n",
      "18s - loss: 1.6677 - act_output_loss: 0.6654 - time_output_loss: 1.0024 - val_loss: 1.5778 - val_act_output_loss: 0.6048 - val_time_output_loss: 0.9730\n",
      "Epoch 6/500\n",
      "18s - loss: 1.6627 - act_output_loss: 0.6624 - time_output_loss: 1.0003 - val_loss: 1.5561 - val_act_output_loss: 0.5848 - val_time_output_loss: 0.9713\n",
      "Epoch 7/500\n",
      "20s - loss: 1.6601 - act_output_loss: 0.6600 - time_output_loss: 1.0002 - val_loss: 1.5337 - val_act_output_loss: 0.5722 - val_time_output_loss: 0.9614\n",
      "Epoch 8/500\n",
      "21s - loss: 1.6572 - act_output_loss: 0.6576 - time_output_loss: 0.9996 - val_loss: 1.5385 - val_act_output_loss: 0.5704 - val_time_output_loss: 0.9681\n",
      "Epoch 9/500\n",
      "20s - loss: 1.6570 - act_output_loss: 0.6582 - time_output_loss: 0.9988 - val_loss: 1.5357 - val_act_output_loss: 0.5613 - val_time_output_loss: 0.9743\n",
      "Epoch 10/500\n",
      "21s - loss: 1.6418 - act_output_loss: 0.6458 - time_output_loss: 0.9960 - val_loss: 1.5392 - val_act_output_loss: 0.5850 - val_time_output_loss: 0.9542\n",
      "Epoch 11/500\n",
      "22s - loss: 1.6415 - act_output_loss: 0.6453 - time_output_loss: 0.9962 - val_loss: 1.5350 - val_act_output_loss: 0.5707 - val_time_output_loss: 0.9643\n",
      "Epoch 12/500\n",
      "22s - loss: 1.6364 - act_output_loss: 0.6436 - time_output_loss: 0.9928 - val_loss: 1.5221 - val_act_output_loss: 0.5681 - val_time_output_loss: 0.9540\n",
      "Epoch 13/500\n",
      "24s - loss: 1.6480 - act_output_loss: 0.6503 - time_output_loss: 0.9976 - val_loss: 1.5428 - val_act_output_loss: 0.5734 - val_time_output_loss: 0.9694\n",
      "Epoch 14/500\n",
      "28s - loss: 1.6374 - act_output_loss: 0.6436 - time_output_loss: 0.9937 - val_loss: 1.5239 - val_act_output_loss: 0.5634 - val_time_output_loss: 0.9605\n",
      "Epoch 15/500\n",
      "28s - loss: 1.6469 - act_output_loss: 0.6469 - time_output_loss: 1.0000 - val_loss: 1.5374 - val_act_output_loss: 0.5658 - val_time_output_loss: 0.9717\n",
      "Epoch 16/500\n",
      "24s - loss: 1.6372 - act_output_loss: 0.6425 - time_output_loss: 0.9947 - val_loss: 1.5467 - val_act_output_loss: 0.5874 - val_time_output_loss: 0.9594\n",
      "Epoch 17/500\n",
      "22s - loss: 1.6430 - act_output_loss: 0.6476 - time_output_loss: 0.9954 - val_loss: 1.5387 - val_act_output_loss: 0.5769 - val_time_output_loss: 0.9618\n",
      "Epoch 18/500\n",
      "20s - loss: 1.6381 - act_output_loss: 0.6435 - time_output_loss: 0.9945 - val_loss: 1.5330 - val_act_output_loss: 0.5703 - val_time_output_loss: 0.9627\n",
      "Epoch 19/500\n",
      "19s - loss: 1.6431 - act_output_loss: 0.6503 - time_output_loss: 0.9928 - val_loss: 1.5457 - val_act_output_loss: 0.5654 - val_time_output_loss: 0.9803\n",
      "Epoch 20/500\n"
     ]
    }
   ],
   "source": [
    "# build the model: \n",
    "print('Build model...')\n",
    "main_input = Input(shape=(maxlen, num_features), name='main_input')\n",
    "\n",
    "# train a 2-layer LSTM with one shared layer\n",
    "l1 = LSTM(100, kernel_initializer=\"glorot_uniform\", dropout=0.2, return_sequences=True, implementation=2)(main_input) # the shared layer\n",
    "b1 = BatchNormalization()(l1)\n",
    "\n",
    "l2_1 = LSTM(100, kernel_initializer=\"glorot_uniform\", dropout=0.2, return_sequences=False, implementation=2)(b1) # the layer specialized in activity prediction\n",
    "b2_1 = BatchNormalization()(l2_1)\n",
    "l2_2 = LSTM(100, kernel_initializer=\"glorot_uniform\", dropout=0.2, return_sequences=False, implementation=2)(b1) # the layer specialized in time prediction\n",
    "b2_2 = BatchNormalization()(l2_2)\n",
    "\n",
    "act_output = Dense(len(targetchartoindice), kernel_initializer=\"glorot_uniform\", name=\"act_output\", activation=\"softmax\")(b2_1)\n",
    "time_output = Dense(1, kernel_initializer=\"glorot_uniform\", name=\"time_output\")(b2_2)\n",
    "\n",
    "model = Model(inputs=[main_input], outputs=[act_output, time_output])\n",
    "\n",
    "opt = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004, clipvalue=3)\n",
    "\n",
    "model.compile(loss={'act_output':'categorical_crossentropy', 'time_output':'mae'}, optimizer=opt)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=42)\n",
    "model_checkpoint = ModelCheckpoint(args.outputdir + 'model_{epoch:02d}-{val_loss:.2f}.h5', \n",
    "                                   monitor='val_loss', verbose=0, save_best_only=True, \n",
    "                                   save_weights_only=False, mode='auto')\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=0, \n",
    "                               mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)\n",
    "history = History()\n",
    "\n",
    "model.fit(X, {'act_output':y_a, 'time_output':y_t}, validation_split=0.2, verbose=2, \n",
    "          callbacks=[early_stopping, model_checkpoint, lr_reducer, history], batch_size=maxlen, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# summarize history for activity accuracy\n",
    "plt.plot(history.history['act_output_loss'])\n",
    "plt.plot(history.history['val_act_output_loss'])\n",
    "plt.title('Activity accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# summarize history for time loss\n",
    "plt.plot(history.history['time_output_loss'])\n",
    "plt.plot(history.history['val_time_output_loss'])\n",
    "plt.title('Time loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# summarize history for model loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model keras 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "# build the model: \n",
    "print('Build model...')\n",
    "main_input = Input(shape=(maxlen, num_features), name='main_input')\n",
    "\n",
    "# train a 2-layer LSTM with one shared layer\n",
    "l1 = LSTM(100, consume_less='gpu', init='glorot_uniform', return_sequences=True, dropout_W=0.2)(main_input) # the shared layer\n",
    "b1 = BatchNormalization()(l1)\n",
    "\n",
    "l2_1 = LSTM(100, consume_less='gpu', init='glorot_uniform', return_sequences=False, dropout_W=0.2)(b1) # the layer specialized in activity prediction\n",
    "b2_1 = BatchNormalization()(l2_1)\n",
    "l2_2 = LSTM(100, consume_less='gpu', init='glorot_uniform', return_sequences=False, dropout_W=0.2)(b1) # the layer specialized in time prediction\n",
    "b2_2 = BatchNormalization()(l2_2)\n",
    "\n",
    "act_output = Dense(len(targetchartoindice), activation='softmax', init='glorot_uniform', name='act_output')(b2_1)\n",
    "time_output = Dense(1, init='glorot_uniform', name='time_output')(b2_2)\n",
    "\n",
    "model = Model(input=[main_input], output=[act_output, time_output])\n",
    "\n",
    "opt = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004, clipvalue=3)\n",
    "\n",
    "model.compile(loss={'act_output':'categorical_crossentropy', 'time_output':'mae'}, optimizer=opt)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=42)\n",
    "model_checkpoint = ModelCheckpoint(args.outputdir + 'model_{epoch:02d}-{val_loss:.2f}.h5', \n",
    "                                   monitor='val_loss', verbose=0, save_best_only=True, \n",
    "                                   save_weights_only=False, mode='auto')\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=0, \n",
    "                               mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "model.fit(X, {'act_output':y_a, 'time_output':y_t}, validation_split=0.2, verbose=2, \n",
    "          callbacks=[early_stopping, model_checkpoint, lr_reducer], batch_size=maxlen, nb_epoch=500)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
