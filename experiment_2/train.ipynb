{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Modification:**\n",
    "- Apply attention for time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Input, Reshape, Flatten, merge\n",
    "from keras import optimizers, regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "name = 'helpdesk'\n",
    "sub_name = 'activity-time' #activity-time\n",
    "args = {\n",
    "    'inputdir': '../input/{}/'.format(name),   \n",
    "    'outputdir': './output_files/{0}_{1}/'.format(name, sub_name)\n",
    "}\n",
    "\n",
    "args = argparse.Namespace(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(args.outputdir):\n",
    "    os.makedirs(args.outputdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(args.inputdir + 'parameters.pkl', \"rb\") as f:\n",
    "    maxlen = pickle.load(f)\n",
    "    num_features = pickle.load(f)\n",
    "    chartoindice = pickle.load(f)\n",
    "    targetchartoindice = pickle.load(f)\n",
    "    divisor = pickle.load(f)\n",
    "    divisor2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(args.inputdir + 'preprocessed_data.pkl', \"rb\") as f:\n",
    "    X = pickle.load(f)\n",
    "    y_a = pickle.load(f)\n",
    "    y_t = pickle.load(f)\n",
    "    X_test = pickle.load(f)\n",
    "    y_a_test = pickle.load(f)\n",
    "    y_t_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers, regularizers, constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializers.get('normal')\n",
    "        #self.input_spec = [InputSpec(ndim=3)]\n",
    "        super(AttLayer, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        #self.W = self.init((input_shape[-1],1))\n",
    "        self.W = K.variable(self.init((input_shape[-1],)))\n",
    "        #self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "\n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "\n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model attention for activity and output...\n",
      "Train on 7344 samples, validate on 1837 samples\n",
      "Epoch 1/500\n",
      "19s - loss: 2.2809 - act_output_loss: 1.1812 - time_output_loss: 1.0997 - val_loss: 2.2867 - val_act_output_loss: 1.2212 - val_time_output_loss: 1.0655\n",
      "Epoch 2/500\n",
      "19s - loss: 1.8835 - act_output_loss: 0.8446 - time_output_loss: 1.0389 - val_loss: 2.6866 - val_act_output_loss: 1.6836 - val_time_output_loss: 1.0030\n",
      "Epoch 3/500\n",
      "18s - loss: 1.7011 - act_output_loss: 0.6859 - time_output_loss: 1.0153 - val_loss: 1.7682 - val_act_output_loss: 0.6841 - val_time_output_loss: 1.0841\n",
      "Epoch 4/500\n",
      "18s - loss: 1.6656 - act_output_loss: 0.6581 - time_output_loss: 1.0075 - val_loss: 1.8977 - val_act_output_loss: 0.8886 - val_time_output_loss: 1.0091\n",
      "Epoch 5/500\n",
      "18s - loss: 1.6526 - act_output_loss: 0.6516 - time_output_loss: 1.0010 - val_loss: 1.7249 - val_act_output_loss: 0.7515 - val_time_output_loss: 0.9734\n",
      "Epoch 6/500\n",
      "19s - loss: 1.6443 - act_output_loss: 0.6499 - time_output_loss: 0.9944 - val_loss: 2.5079 - val_act_output_loss: 1.5287 - val_time_output_loss: 0.9792\n",
      "Epoch 7/500\n",
      "18s - loss: 1.6365 - act_output_loss: 0.6435 - time_output_loss: 0.9929 - val_loss: 1.5669 - val_act_output_loss: 0.5920 - val_time_output_loss: 0.9749\n",
      "Epoch 8/500\n",
      "19s - loss: 1.6342 - act_output_loss: 0.6456 - time_output_loss: 0.9886 - val_loss: 2.3402 - val_act_output_loss: 1.3772 - val_time_output_loss: 0.9629\n",
      "Epoch 9/500\n",
      "18s - loss: 1.6284 - act_output_loss: 0.6375 - time_output_loss: 0.9909 - val_loss: 1.5400 - val_act_output_loss: 0.5709 - val_time_output_loss: 0.9691\n",
      "Epoch 10/500\n",
      "18s - loss: 1.6250 - act_output_loss: 0.6346 - time_output_loss: 0.9904 - val_loss: 1.5895 - val_act_output_loss: 0.6204 - val_time_output_loss: 0.9691\n",
      "Epoch 11/500\n",
      "18s - loss: 1.6253 - act_output_loss: 0.6352 - time_output_loss: 0.9901 - val_loss: 2.5489 - val_act_output_loss: 1.5800 - val_time_output_loss: 0.9689\n",
      "Epoch 12/500\n",
      "18s - loss: 1.6271 - act_output_loss: 0.6395 - time_output_loss: 0.9876 - val_loss: 1.9521 - val_act_output_loss: 0.9909 - val_time_output_loss: 0.9613\n",
      "Epoch 13/500\n",
      "18s - loss: 1.6392 - act_output_loss: 0.6491 - time_output_loss: 0.9901 - val_loss: 1.7892 - val_act_output_loss: 0.8209 - val_time_output_loss: 0.9683\n",
      "Epoch 14/500\n",
      "19s - loss: 1.6378 - act_output_loss: 0.6467 - time_output_loss: 0.9911 - val_loss: 1.8694 - val_act_output_loss: 0.9030 - val_time_output_loss: 0.9664\n",
      "Epoch 15/500\n",
      "18s - loss: 1.6306 - act_output_loss: 0.6441 - time_output_loss: 0.9865 - val_loss: 1.6081 - val_act_output_loss: 0.6509 - val_time_output_loss: 0.9571\n",
      "Epoch 16/500\n",
      "18s - loss: 1.6243 - act_output_loss: 0.6381 - time_output_loss: 0.9863 - val_loss: 1.5272 - val_act_output_loss: 0.5685 - val_time_output_loss: 0.9587\n",
      "Epoch 17/500\n",
      "18s - loss: 1.6318 - act_output_loss: 0.6417 - time_output_loss: 0.9901 - val_loss: 1.5420 - val_act_output_loss: 0.5838 - val_time_output_loss: 0.9582\n",
      "Epoch 18/500\n",
      "18s - loss: 1.6161 - act_output_loss: 0.6282 - time_output_loss: 0.9879 - val_loss: 5.6248 - val_act_output_loss: 4.6656 - val_time_output_loss: 0.9592\n",
      "Epoch 19/500\n",
      "17s - loss: 1.6160 - act_output_loss: 0.6254 - time_output_loss: 0.9906 - val_loss: 1.7021 - val_act_output_loss: 0.7343 - val_time_output_loss: 0.9678\n",
      "Epoch 20/500\n",
      "18s - loss: 1.6039 - act_output_loss: 0.6166 - time_output_loss: 0.9873 - val_loss: 1.5336 - val_act_output_loss: 0.5730 - val_time_output_loss: 0.9607\n",
      "Epoch 21/500\n",
      "18s - loss: 1.6059 - act_output_loss: 0.6153 - time_output_loss: 0.9906 - val_loss: 1.6816 - val_act_output_loss: 0.7171 - val_time_output_loss: 0.9645\n",
      "Epoch 22/500\n",
      "18s - loss: 1.6167 - act_output_loss: 0.6259 - time_output_loss: 0.9908 - val_loss: 1.5297 - val_act_output_loss: 0.5722 - val_time_output_loss: 0.9575\n",
      "Epoch 23/500\n",
      "17s - loss: 1.6083 - act_output_loss: 0.6181 - time_output_loss: 0.9902 - val_loss: 1.6489 - val_act_output_loss: 0.6886 - val_time_output_loss: 0.9603\n",
      "Epoch 24/500\n",
      "18s - loss: 1.6088 - act_output_loss: 0.6207 - time_output_loss: 0.9882 - val_loss: 1.5258 - val_act_output_loss: 0.5634 - val_time_output_loss: 0.9624\n",
      "Epoch 25/500\n",
      "18s - loss: 1.6060 - act_output_loss: 0.6224 - time_output_loss: 0.9836 - val_loss: 1.5282 - val_act_output_loss: 0.5720 - val_time_output_loss: 0.9562\n",
      "Epoch 26/500\n",
      "18s - loss: 1.6296 - act_output_loss: 0.6389 - time_output_loss: 0.9906 - val_loss: 1.5344 - val_act_output_loss: 0.5788 - val_time_output_loss: 0.9556\n",
      "Epoch 27/500\n",
      "17s - loss: 1.6047 - act_output_loss: 0.6166 - time_output_loss: 0.9881 - val_loss: 1.5277 - val_act_output_loss: 0.5673 - val_time_output_loss: 0.9604\n",
      "Epoch 28/500\n",
      "18s - loss: 1.6153 - act_output_loss: 0.6248 - time_output_loss: 0.9906 - val_loss: 1.5272 - val_act_output_loss: 0.5664 - val_time_output_loss: 0.9608\n",
      "Epoch 29/500\n",
      "18s - loss: 1.6008 - act_output_loss: 0.6189 - time_output_loss: 0.9819 - val_loss: 1.5407 - val_act_output_loss: 0.5849 - val_time_output_loss: 0.9558\n",
      "Epoch 30/500\n",
      "19s - loss: 1.6126 - act_output_loss: 0.6248 - time_output_loss: 0.9879 - val_loss: 1.5209 - val_act_output_loss: 0.5660 - val_time_output_loss: 0.9549\n",
      "Epoch 31/500\n",
      "18s - loss: 1.6016 - act_output_loss: 0.6175 - time_output_loss: 0.9842 - val_loss: 1.5259 - val_act_output_loss: 0.5696 - val_time_output_loss: 0.9562\n",
      "Epoch 32/500\n",
      "19s - loss: 1.6000 - act_output_loss: 0.6149 - time_output_loss: 0.9851 - val_loss: 1.5292 - val_act_output_loss: 0.5679 - val_time_output_loss: 0.9613\n",
      "Epoch 33/500\n",
      "18s - loss: 1.5971 - act_output_loss: 0.6111 - time_output_loss: 0.9860 - val_loss: 1.5630 - val_act_output_loss: 0.5964 - val_time_output_loss: 0.9666\n",
      "Epoch 34/500\n",
      "18s - loss: 1.6052 - act_output_loss: 0.6162 - time_output_loss: 0.9890 - val_loss: 1.5146 - val_act_output_loss: 0.5627 - val_time_output_loss: 0.9519\n",
      "Epoch 35/500\n",
      "19s - loss: 1.6047 - act_output_loss: 0.6160 - time_output_loss: 0.9887 - val_loss: 1.5427 - val_act_output_loss: 0.5784 - val_time_output_loss: 0.9643\n",
      "Epoch 36/500\n",
      "23s - loss: 1.6058 - act_output_loss: 0.6189 - time_output_loss: 0.9869 - val_loss: 1.5262 - val_act_output_loss: 0.5680 - val_time_output_loss: 0.9582\n",
      "Epoch 37/500\n",
      "23s - loss: 1.5998 - act_output_loss: 0.6144 - time_output_loss: 0.9854 - val_loss: 1.5220 - val_act_output_loss: 0.5654 - val_time_output_loss: 0.9566\n",
      "Epoch 38/500\n",
      "21s - loss: 1.6037 - act_output_loss: 0.6163 - time_output_loss: 0.9873 - val_loss: 1.5455 - val_act_output_loss: 0.5825 - val_time_output_loss: 0.9630\n",
      "Epoch 39/500\n",
      "36s - loss: 1.6088 - act_output_loss: 0.6210 - time_output_loss: 0.9878 - val_loss: 1.5290 - val_act_output_loss: 0.5638 - val_time_output_loss: 0.9652\n",
      "Epoch 40/500\n",
      "19s - loss: 1.5995 - act_output_loss: 0.6128 - time_output_loss: 0.9867 - val_loss: 1.5274 - val_act_output_loss: 0.5696 - val_time_output_loss: 0.9578\n",
      "Epoch 41/500\n",
      "19s - loss: 1.6017 - act_output_loss: 0.6148 - time_output_loss: 0.9869 - val_loss: 1.5250 - val_act_output_loss: 0.5642 - val_time_output_loss: 0.9609\n",
      "Epoch 42/500\n",
      "24s - loss: 1.5907 - act_output_loss: 0.6060 - time_output_loss: 0.9847 - val_loss: 1.5272 - val_act_output_loss: 0.5700 - val_time_output_loss: 0.9572\n",
      "Epoch 43/500\n",
      "19s - loss: 1.5906 - act_output_loss: 0.6063 - time_output_loss: 0.9844 - val_loss: 1.5317 - val_act_output_loss: 0.5696 - val_time_output_loss: 0.9622\n",
      "Epoch 44/500\n",
      "19s - loss: 1.5900 - act_output_loss: 0.6040 - time_output_loss: 0.9860 - val_loss: 1.5396 - val_act_output_loss: 0.5743 - val_time_output_loss: 0.9654\n",
      "Epoch 45/500\n",
      "18s - loss: 1.5972 - act_output_loss: 0.6096 - time_output_loss: 0.9876 - val_loss: 1.5320 - val_act_output_loss: 0.5699 - val_time_output_loss: 0.9621\n",
      "Epoch 46/500\n",
      "18s - loss: 1.5852 - act_output_loss: 0.6026 - time_output_loss: 0.9825 - val_loss: 1.5256 - val_act_output_loss: 0.5671 - val_time_output_loss: 0.9585\n",
      "Epoch 47/500\n",
      "18s - loss: 1.5775 - act_output_loss: 0.5969 - time_output_loss: 0.9806 - val_loss: 1.5225 - val_act_output_loss: 0.5691 - val_time_output_loss: 0.9534\n",
      "Epoch 48/500\n",
      "18s - loss: 1.5857 - act_output_loss: 0.6010 - time_output_loss: 0.9847 - val_loss: 1.5233 - val_act_output_loss: 0.5665 - val_time_output_loss: 0.9568\n",
      "Epoch 49/500\n",
      "18s - loss: 1.5788 - act_output_loss: 0.5952 - time_output_loss: 0.9836 - val_loss: 1.5207 - val_act_output_loss: 0.5628 - val_time_output_loss: 0.9579\n",
      "Epoch 50/500\n",
      "18s - loss: 1.5667 - act_output_loss: 0.5873 - time_output_loss: 0.9794 - val_loss: 1.5286 - val_act_output_loss: 0.5659 - val_time_output_loss: 0.9627\n",
      "Epoch 51/500\n",
      "18s - loss: 1.5792 - act_output_loss: 0.5967 - time_output_loss: 0.9825 - val_loss: 1.5269 - val_act_output_loss: 0.5677 - val_time_output_loss: 0.9592\n",
      "Epoch 52/500\n",
      "18s - loss: 1.5686 - act_output_loss: 0.5893 - time_output_loss: 0.9793 - val_loss: 1.5248 - val_act_output_loss: 0.5688 - val_time_output_loss: 0.9561\n",
      "Epoch 53/500\n",
      "18s - loss: 1.5690 - act_output_loss: 0.5902 - time_output_loss: 0.9788 - val_loss: 1.5210 - val_act_output_loss: 0.5697 - val_time_output_loss: 0.9513\n",
      "Epoch 54/500\n",
      "18s - loss: 1.5787 - act_output_loss: 0.5956 - time_output_loss: 0.9832 - val_loss: 1.5259 - val_act_output_loss: 0.5631 - val_time_output_loss: 0.9629\n",
      "Epoch 55/500\n",
      "18s - loss: 1.5798 - act_output_loss: 0.5922 - time_output_loss: 0.9876 - val_loss: 1.5316 - val_act_output_loss: 0.5675 - val_time_output_loss: 0.9642\n",
      "Epoch 56/500\n",
      "18s - loss: 1.5688 - act_output_loss: 0.5923 - time_output_loss: 0.9765 - val_loss: 1.5198 - val_act_output_loss: 0.5623 - val_time_output_loss: 0.9574\n",
      "Epoch 57/500\n",
      "18s - loss: 1.5727 - act_output_loss: 0.5914 - time_output_loss: 0.9813 - val_loss: 1.5245 - val_act_output_loss: 0.5668 - val_time_output_loss: 0.9577\n",
      "Epoch 58/500\n",
      "18s - loss: 1.5677 - act_output_loss: 0.5858 - time_output_loss: 0.9819 - val_loss: 1.5200 - val_act_output_loss: 0.5648 - val_time_output_loss: 0.9552\n",
      "Epoch 59/500\n",
      "18s - loss: 1.5705 - act_output_loss: 0.5898 - time_output_loss: 0.9806 - val_loss: 1.5203 - val_act_output_loss: 0.5649 - val_time_output_loss: 0.9553\n",
      "Epoch 60/500\n",
      "18s - loss: 1.5635 - act_output_loss: 0.5859 - time_output_loss: 0.9776 - val_loss: 1.5286 - val_act_output_loss: 0.5697 - val_time_output_loss: 0.9589\n",
      "Epoch 61/500\n",
      "19s - loss: 1.5672 - act_output_loss: 0.5886 - time_output_loss: 0.9785 - val_loss: 1.5291 - val_act_output_loss: 0.5648 - val_time_output_loss: 0.9643\n",
      "Epoch 62/500\n",
      "18s - loss: 1.5714 - act_output_loss: 0.5892 - time_output_loss: 0.9822 - val_loss: 1.5204 - val_act_output_loss: 0.5651 - val_time_output_loss: 0.9553\n",
      "Epoch 63/500\n",
      "18s - loss: 1.5713 - act_output_loss: 0.5875 - time_output_loss: 0.9839 - val_loss: 1.5206 - val_act_output_loss: 0.5646 - val_time_output_loss: 0.9561\n",
      "Epoch 64/500\n",
      "18s - loss: 1.5729 - act_output_loss: 0.5929 - time_output_loss: 0.9800 - val_loss: 1.5206 - val_act_output_loss: 0.5656 - val_time_output_loss: 0.9549\n",
      "Epoch 65/500\n",
      "18s - loss: 1.5615 - act_output_loss: 0.5827 - time_output_loss: 0.9787 - val_loss: 1.5265 - val_act_output_loss: 0.5670 - val_time_output_loss: 0.9595\n",
      "Epoch 66/500\n",
      "18s - loss: 1.5662 - act_output_loss: 0.5843 - time_output_loss: 0.9819 - val_loss: 1.5236 - val_act_output_loss: 0.5664 - val_time_output_loss: 0.9572\n",
      "Epoch 67/500\n",
      "18s - loss: 1.5682 - act_output_loss: 0.5842 - time_output_loss: 0.9840 - val_loss: 1.5226 - val_act_output_loss: 0.5655 - val_time_output_loss: 0.9571\n",
      "Epoch 68/500\n",
      "18s - loss: 1.5691 - act_output_loss: 0.5858 - time_output_loss: 0.9834 - val_loss: 1.5223 - val_act_output_loss: 0.5638 - val_time_output_loss: 0.9585\n",
      "Epoch 69/500\n",
      "18s - loss: 1.5617 - act_output_loss: 0.5863 - time_output_loss: 0.9754 - val_loss: 1.5227 - val_act_output_loss: 0.5646 - val_time_output_loss: 0.9581\n",
      "Epoch 70/500\n",
      "18s - loss: 1.5639 - act_output_loss: 0.5855 - time_output_loss: 0.9783 - val_loss: 1.5230 - val_act_output_loss: 0.5637 - val_time_output_loss: 0.9593\n",
      "Epoch 71/500\n",
      "18s - loss: 1.5587 - act_output_loss: 0.5787 - time_output_loss: 0.9800 - val_loss: 1.5280 - val_act_output_loss: 0.5663 - val_time_output_loss: 0.9616\n",
      "Epoch 72/500\n",
      "18s - loss: 1.5597 - act_output_loss: 0.5799 - time_output_loss: 0.9798 - val_loss: 1.5225 - val_act_output_loss: 0.5652 - val_time_output_loss: 0.9574\n",
      "Epoch 73/500\n",
      "18s - loss: 1.5575 - act_output_loss: 0.5780 - time_output_loss: 0.9795 - val_loss: 1.5245 - val_act_output_loss: 0.5650 - val_time_output_loss: 0.9595\n",
      "Epoch 74/500\n",
      "18s - loss: 1.5594 - act_output_loss: 0.5826 - time_output_loss: 0.9768 - val_loss: 1.5227 - val_act_output_loss: 0.5647 - val_time_output_loss: 0.9579\n",
      "Epoch 75/500\n",
      "18s - loss: 1.5596 - act_output_loss: 0.5791 - time_output_loss: 0.9804 - val_loss: 1.5247 - val_act_output_loss: 0.5663 - val_time_output_loss: 0.9584\n",
      "Epoch 76/500\n",
      "18s - loss: 1.5630 - act_output_loss: 0.5825 - time_output_loss: 0.9804 - val_loss: 1.5252 - val_act_output_loss: 0.5662 - val_time_output_loss: 0.9591\n",
      "Epoch 77/500\n",
      "19s - loss: 1.5562 - act_output_loss: 0.5780 - time_output_loss: 0.9781 - val_loss: 1.5244 - val_act_output_loss: 0.5659 - val_time_output_loss: 0.9586\n",
      "Epoch 78/500\n",
      "18s - loss: 1.5595 - act_output_loss: 0.5794 - time_output_loss: 0.9801 - val_loss: 1.5245 - val_act_output_loss: 0.5662 - val_time_output_loss: 0.9583\n",
      "Epoch 79/500\n",
      "18s - loss: 1.5604 - act_output_loss: 0.5788 - time_output_loss: 0.9816 - val_loss: 1.5251 - val_act_output_loss: 0.5656 - val_time_output_loss: 0.9595\n",
      "Epoch 80/500\n",
      "18s - loss: 1.5533 - act_output_loss: 0.5771 - time_output_loss: 0.9761 - val_loss: 1.5235 - val_act_output_loss: 0.5658 - val_time_output_loss: 0.9577\n",
      "Epoch 81/500\n",
      "18s - loss: 1.5538 - act_output_loss: 0.5757 - time_output_loss: 0.9781 - val_loss: 1.5234 - val_act_output_loss: 0.5647 - val_time_output_loss: 0.9587\n",
      "Epoch 82/500\n",
      "18s - loss: 1.5601 - act_output_loss: 0.5817 - time_output_loss: 0.9785 - val_loss: 1.5249 - val_act_output_loss: 0.5640 - val_time_output_loss: 0.9609\n",
      "Epoch 83/500\n",
      "18s - loss: 1.5525 - act_output_loss: 0.5717 - time_output_loss: 0.9808 - val_loss: 1.5238 - val_act_output_loss: 0.5641 - val_time_output_loss: 0.9597\n",
      "Epoch 84/500\n",
      "18s - loss: 1.5574 - act_output_loss: 0.5769 - time_output_loss: 0.9805 - val_loss: 1.5251 - val_act_output_loss: 0.5646 - val_time_output_loss: 0.9605\n",
      "Epoch 85/500\n",
      "18s - loss: 1.5582 - act_output_loss: 0.5769 - time_output_loss: 0.9814 - val_loss: 1.5272 - val_act_output_loss: 0.5667 - val_time_output_loss: 0.9605\n",
      "Epoch 86/500\n",
      "18s - loss: 1.5585 - act_output_loss: 0.5750 - time_output_loss: 0.9835 - val_loss: 1.5271 - val_act_output_loss: 0.5664 - val_time_output_loss: 0.9607\n",
      "Epoch 87/500\n",
      "18s - loss: 1.5595 - act_output_loss: 0.5787 - time_output_loss: 0.9808 - val_loss: 1.5258 - val_act_output_loss: 0.5668 - val_time_output_loss: 0.9590\n",
      "Epoch 88/500\n",
      "18s - loss: 1.5515 - act_output_loss: 0.5732 - time_output_loss: 0.9783 - val_loss: 1.5249 - val_act_output_loss: 0.5666 - val_time_output_loss: 0.9583\n",
      "Epoch 89/500\n",
      "18s - loss: 1.5567 - act_output_loss: 0.5742 - time_output_loss: 0.9825 - val_loss: 1.5236 - val_act_output_loss: 0.5662 - val_time_output_loss: 0.9574\n",
      "Epoch 90/500\n",
      "18s - loss: 1.5614 - act_output_loss: 0.5837 - time_output_loss: 0.9776 - val_loss: 1.5251 - val_act_output_loss: 0.5669 - val_time_output_loss: 0.9582\n",
      "Epoch 91/500\n",
      "18s - loss: 1.5555 - act_output_loss: 0.5758 - time_output_loss: 0.9796 - val_loss: 1.5254 - val_act_output_loss: 0.5661 - val_time_output_loss: 0.9593\n",
      "Epoch 92/500\n",
      "18s - loss: 1.5476 - act_output_loss: 0.5708 - time_output_loss: 0.9768 - val_loss: 1.5243 - val_act_output_loss: 0.5657 - val_time_output_loss: 0.9586\n",
      "Epoch 93/500\n",
      "19s - loss: 1.5569 - act_output_loss: 0.5762 - time_output_loss: 0.9807 - val_loss: 1.5273 - val_act_output_loss: 0.5657 - val_time_output_loss: 0.9616\n",
      "Epoch 94/500\n",
      "18s - loss: 1.5575 - act_output_loss: 0.5799 - time_output_loss: 0.9775 - val_loss: 1.5244 - val_act_output_loss: 0.5654 - val_time_output_loss: 0.9590\n",
      "Epoch 95/500\n",
      "18s - loss: 1.5542 - act_output_loss: 0.5726 - time_output_loss: 0.9816 - val_loss: 1.5249 - val_act_output_loss: 0.5654 - val_time_output_loss: 0.9594\n",
      "Epoch 96/500\n",
      "18s - loss: 1.5667 - act_output_loss: 0.5826 - time_output_loss: 0.9841 - val_loss: 1.5255 - val_act_output_loss: 0.5656 - val_time_output_loss: 0.9600\n",
      "Epoch 97/500\n",
      "18s - loss: 1.5580 - act_output_loss: 0.5792 - time_output_loss: 0.9787 - val_loss: 1.5239 - val_act_output_loss: 0.5651 - val_time_output_loss: 0.9588\n",
      "Epoch 98/500\n",
      "18s - loss: 1.5625 - act_output_loss: 0.5800 - time_output_loss: 0.9825 - val_loss: 1.5239 - val_act_output_loss: 0.5655 - val_time_output_loss: 0.9584\n",
      "Epoch 99/500\n",
      "18s - loss: 1.5519 - act_output_loss: 0.5747 - time_output_loss: 0.9771 - val_loss: 1.5243 - val_act_output_loss: 0.5654 - val_time_output_loss: 0.9589\n",
      "Epoch 100/500\n",
      "18s - loss: 1.5612 - act_output_loss: 0.5814 - time_output_loss: 0.9798 - val_loss: 1.5236 - val_act_output_loss: 0.5651 - val_time_output_loss: 0.9585\n",
      "Epoch 101/500\n",
      "18s - loss: 1.5601 - act_output_loss: 0.5792 - time_output_loss: 0.9809 - val_loss: 1.5246 - val_act_output_loss: 0.5653 - val_time_output_loss: 0.9593\n",
      "Epoch 102/500\n",
      "18s - loss: 1.5617 - act_output_loss: 0.5800 - time_output_loss: 0.9817 - val_loss: 1.5242 - val_act_output_loss: 0.5654 - val_time_output_loss: 0.9588\n",
      "Epoch 103/500\n",
      "18s - loss: 1.5466 - act_output_loss: 0.5723 - time_output_loss: 0.9743 - val_loss: 1.5234 - val_act_output_loss: 0.5655 - val_time_output_loss: 0.9579\n",
      "Epoch 104/500\n",
      "18s - loss: 1.5588 - act_output_loss: 0.5795 - time_output_loss: 0.9793 - val_loss: 1.5240 - val_act_output_loss: 0.5657 - val_time_output_loss: 0.9583\n",
      "Epoch 105/500\n",
      "18s - loss: 1.5549 - act_output_loss: 0.5755 - time_output_loss: 0.9794 - val_loss: 1.5237 - val_act_output_loss: 0.5654 - val_time_output_loss: 0.9584\n",
      "Epoch 106/500\n",
      "18s - loss: 1.5490 - act_output_loss: 0.5697 - time_output_loss: 0.9793 - val_loss: 1.5242 - val_act_output_loss: 0.5650 - val_time_output_loss: 0.9592\n",
      "Epoch 107/500\n",
      "18s - loss: 1.5577 - act_output_loss: 0.5748 - time_output_loss: 0.9829 - val_loss: 1.5228 - val_act_output_loss: 0.5653 - val_time_output_loss: 0.9575\n",
      "Epoch 108/500\n",
      "18s - loss: 1.5578 - act_output_loss: 0.5775 - time_output_loss: 0.9803 - val_loss: 1.5238 - val_act_output_loss: 0.5654 - val_time_output_loss: 0.9584\n",
      "Epoch 109/500\n",
      "19s - loss: 1.5608 - act_output_loss: 0.5780 - time_output_loss: 0.9828 - val_loss: 1.5244 - val_act_output_loss: 0.5653 - val_time_output_loss: 0.9591\n",
      "Epoch 110/500\n",
      "18s - loss: 1.5566 - act_output_loss: 0.5771 - time_output_loss: 0.9795 - val_loss: 1.5226 - val_act_output_loss: 0.5651 - val_time_output_loss: 0.9575\n",
      "Epoch 111/500\n",
      "18s - loss: 1.5530 - act_output_loss: 0.5723 - time_output_loss: 0.9807 - val_loss: 1.5234 - val_act_output_loss: 0.5653 - val_time_output_loss: 0.9581\n",
      "Epoch 112/500\n",
      "18s - loss: 1.5662 - act_output_loss: 0.5841 - time_output_loss: 0.9821 - val_loss: 1.5241 - val_act_output_loss: 0.5654 - val_time_output_loss: 0.9587\n",
      "Epoch 113/500\n",
      "18s - loss: 1.5613 - act_output_loss: 0.5823 - time_output_loss: 0.9790 - val_loss: 1.5234 - val_act_output_loss: 0.5652 - val_time_output_loss: 0.9582\n",
      "Epoch 114/500\n",
      "18s - loss: 1.5559 - act_output_loss: 0.5774 - time_output_loss: 0.9785 - val_loss: 1.5227 - val_act_output_loss: 0.5652 - val_time_output_loss: 0.9574\n",
      "Epoch 115/500\n",
      "19s - loss: 1.5534 - act_output_loss: 0.5793 - time_output_loss: 0.9741 - val_loss: 1.5243 - val_act_output_loss: 0.5654 - val_time_output_loss: 0.9590\n",
      "Epoch 116/500\n",
      "18s - loss: 1.5537 - act_output_loss: 0.5734 - time_output_loss: 0.9803 - val_loss: 1.5239 - val_act_output_loss: 0.5652 - val_time_output_loss: 0.9587\n",
      "Epoch 117/500\n",
      "18s - loss: 1.5498 - act_output_loss: 0.5727 - time_output_loss: 0.9771 - val_loss: 1.5233 - val_act_output_loss: 0.5653 - val_time_output_loss: 0.9580\n",
      "Epoch 118/500\n",
      "18s - loss: 1.5478 - act_output_loss: 0.5707 - time_output_loss: 0.9771 - val_loss: 1.5232 - val_act_output_loss: 0.5654 - val_time_output_loss: 0.9578\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f78dfbe4ef0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the model: \n",
    "print('Build model attention for activity and output...')\n",
    "main_input = Input(shape=(maxlen, num_features), name='main_input')\n",
    "\n",
    "# one shared layer\n",
    "l1 = LSTM(100, implementation=2, kernel_initializer=\"glorot_uniform\", dropout=0.2, return_sequences=True)(main_input)\n",
    "b1 = BatchNormalization()(l1)\n",
    "\n",
    "# the layer specialized in activity prediction\n",
    "l2_1 = LSTM(100, implementation=2, kernel_initializer=\"glorot_uniform\", dropout=0.2, return_sequences=True)(b1) \n",
    "b2_1 = BatchNormalization()(l2_1)\n",
    "att_1 = AttLayer()(b2_1)\n",
    "\n",
    "# the layer specialized in time prediction\n",
    "l2_2 = LSTM(100, implementation=2, kernel_initializer=\"glorot_uniform\", dropout=0.2, return_sequences=False)(l1) \n",
    "b2_2 = BatchNormalization()(l2_2)\n",
    "\n",
    "\n",
    "act_output = Dense(len(targetchartoindice), activation=\"softmax\", kernel_initializer=\"glorot_uniform\", name=\"act_output\")(att_1)\n",
    "time_output = Dense(1, kernel_initializer=\"glorot_uniform\", name=\"time_output\")(b2_2)\n",
    "\n",
    "\n",
    "model = Model(inputs=[main_input], outputs=[act_output, time_output])\n",
    "\n",
    "#compilations\n",
    "opt = optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004, clipvalue=3)\n",
    "model.compile(loss={'act_output':'categorical_crossentropy', 'time_output':'mean_absolute_error'}, optimizer=opt)\n",
    "\n",
    "#callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=83)\n",
    "model_checkpoint = ModelCheckpoint(args.outputdir + 'model_{epoch:02d}-{val_loss:.2f}.h5', \n",
    "                                   monitor='val_loss', verbose=0, save_best_only=True, \n",
    "                                   save_weights_only=False, mode='auto')\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, \n",
    "                               verbose=0, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "#fit\n",
    "model.fit(X, {'act_output':y_a, 'time_output':y_t}, validation_split=0.2, verbose=2, \n",
    "          callbacks=[early_stopping, model_checkpoint, lr_reducer], batch_size=16, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "main_input (InputLayer)          (None, 15, 14)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                    (None, 15, 100)       46000       main_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNorm (None, 15, 100)       400         lstm_4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                    (None, 15, 100)       80400       batch_normalization_4[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNorm (None, 15, 100)       400         lstm_5[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                    (None, 100)           80400       lstm_4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "att_layer_2 (AttLayer)           (None, 100)           100         batch_normalization_5[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNorm (None, 100)           400         lstm_6[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "act_output (Dense)               (None, 10)            1010        att_layer_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "time_output (Dense)              (None, 1)             101         batch_normalization_6[0][0]      \n",
      "====================================================================================================\n",
      "Total params: 209,211\n",
      "Trainable params: 208,611\n",
      "Non-trainable params: 600\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "        Attention operation, with a context/query vector, for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "        \"Hierarchical Attention Networks for Document Classification\"\n",
    "        by using a context vector to assist the attention\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(AttentionWithContext())\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = K.dot(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = K.dot(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model attention for activity and output...\n",
      "Train on 7344 samples, validate on 1837 samples\n",
      "Epoch 1/500\n",
      "7s - loss: 7.4331 - act_output_loss: 1.4924 - time_output_loss: 1.0280 - val_loss: 2.7491 - val_act_output_loss: 1.5751 - val_time_output_loss: 1.0092\n",
      "Epoch 2/500\n",
      "8s - loss: 2.4652 - act_output_loss: 1.4198 - time_output_loss: 1.0192 - val_loss: 2.4444 - val_act_output_loss: 1.4307 - val_time_output_loss: 1.0133\n",
      "Epoch 3/500\n",
      "8s - loss: 2.4362 - act_output_loss: 1.4155 - time_output_loss: 1.0196 - val_loss: 2.4473 - val_act_output_loss: 1.4309 - val_time_output_loss: 1.0159\n",
      "Epoch 4/500\n",
      "8s - loss: 2.4366 - act_output_loss: 1.4147 - time_output_loss: 1.0202 - val_loss: 2.4643 - val_act_output_loss: 1.4279 - val_time_output_loss: 1.0359\n",
      "Epoch 5/500\n",
      "9s - loss: 2.4288 - act_output_loss: 1.4104 - time_output_loss: 1.0178 - val_loss: 2.4403 - val_act_output_loss: 1.4287 - val_time_output_loss: 1.0112\n",
      "Epoch 6/500\n",
      "8s - loss: 2.4331 - act_output_loss: 1.4107 - time_output_loss: 1.0207 - val_loss: 2.4366 - val_act_output_loss: 1.4269 - val_time_output_loss: 1.0091\n",
      "Epoch 7/500\n",
      "8s - loss: 2.4273 - act_output_loss: 1.4082 - time_output_loss: 1.0182 - val_loss: 2.4432 - val_act_output_loss: 1.4306 - val_time_output_loss: 1.0118\n",
      "Epoch 8/500\n",
      "8s - loss: 2.4261 - act_output_loss: 1.4078 - time_output_loss: 1.0174 - val_loss: 2.4409 - val_act_output_loss: 1.4274 - val_time_output_loss: 1.0124\n",
      "Epoch 9/500\n",
      "7s - loss: 2.4245 - act_output_loss: 1.4063 - time_output_loss: 1.0173 - val_loss: 2.4412 - val_act_output_loss: 1.4252 - val_time_output_loss: 1.0155\n",
      "Epoch 10/500\n",
      "7s - loss: 2.4232 - act_output_loss: 1.4044 - time_output_loss: 1.0179 - val_loss: 2.4401 - val_act_output_loss: 1.4252 - val_time_output_loss: 1.0133\n",
      "Epoch 11/500\n",
      "7s - loss: 2.4219 - act_output_loss: 1.4026 - time_output_loss: 1.0176 - val_loss: 2.4396 - val_act_output_loss: 1.4263 - val_time_output_loss: 1.0126\n",
      "Epoch 12/500\n",
      "7s - loss: 2.4191 - act_output_loss: 1.4011 - time_output_loss: 1.0169 - val_loss: 2.4384 - val_act_output_loss: 1.4257 - val_time_output_loss: 1.0123\n",
      "Epoch 13/500\n",
      "7s - loss: 2.4165 - act_output_loss: 1.3991 - time_output_loss: 1.0165 - val_loss: 2.4379 - val_act_output_loss: 1.4261 - val_time_output_loss: 1.0116\n",
      "Epoch 14/500\n",
      "7s - loss: 2.4117 - act_output_loss: 1.3948 - time_output_loss: 1.0160 - val_loss: 2.4440 - val_act_output_loss: 1.4244 - val_time_output_loss: 1.0191\n",
      "Epoch 15/500\n",
      "7s - loss: 2.4128 - act_output_loss: 1.3962 - time_output_loss: 1.0160 - val_loss: 2.4339 - val_act_output_loss: 1.4232 - val_time_output_loss: 1.0103\n",
      "Epoch 16/500\n",
      "8s - loss: 2.4136 - act_output_loss: 1.3971 - time_output_loss: 1.0158 - val_loss: 2.4373 - val_act_output_loss: 1.4254 - val_time_output_loss: 1.0104\n",
      "Epoch 17/500\n",
      "7s - loss: 2.4085 - act_output_loss: 1.3929 - time_output_loss: 1.0150 - val_loss: 2.4440 - val_act_output_loss: 1.4255 - val_time_output_loss: 1.0180\n",
      "Epoch 18/500\n",
      "7s - loss: 2.4069 - act_output_loss: 1.3906 - time_output_loss: 1.0153 - val_loss: 2.4324 - val_act_output_loss: 1.4240 - val_time_output_loss: 1.0077\n",
      "Epoch 19/500\n",
      "8s - loss: 2.4045 - act_output_loss: 1.3910 - time_output_loss: 1.0127 - val_loss: 2.4256 - val_act_output_loss: 1.4273 - val_time_output_loss: 0.9975\n",
      "Epoch 20/500\n",
      "8s - loss: 2.4009 - act_output_loss: 1.3891 - time_output_loss: 1.0108 - val_loss: 2.4218 - val_act_output_loss: 1.4269 - val_time_output_loss: 0.9940\n",
      "Epoch 21/500\n",
      "8s - loss: 2.3980 - act_output_loss: 1.3880 - time_output_loss: 1.0092 - val_loss: 2.4229 - val_act_output_loss: 1.4290 - val_time_output_loss: 0.9929\n",
      "Epoch 22/500\n",
      "7s - loss: 2.3986 - act_output_loss: 1.3905 - time_output_loss: 1.0068 - val_loss: 2.4192 - val_act_output_loss: 1.4272 - val_time_output_loss: 0.9908\n",
      "Epoch 23/500\n",
      "8s - loss: 2.3922 - act_output_loss: 1.3846 - time_output_loss: 1.0061 - val_loss: 2.4186 - val_act_output_loss: 1.4270 - val_time_output_loss: 0.9906\n",
      "Epoch 24/500\n",
      "8s - loss: 2.3887 - act_output_loss: 1.3843 - time_output_loss: 1.0024 - val_loss: 2.4234 - val_act_output_loss: 1.4307 - val_time_output_loss: 0.9903\n",
      "Epoch 25/500\n",
      "7s - loss: 2.3901 - act_output_loss: 1.3865 - time_output_loss: 1.0015 - val_loss: 2.4246 - val_act_output_loss: 1.4286 - val_time_output_loss: 0.9938\n",
      "Epoch 26/500\n",
      "7s - loss: 2.3827 - act_output_loss: 1.3813 - time_output_loss: 0.9986 - val_loss: 2.4098 - val_act_output_loss: 1.4268 - val_time_output_loss: 0.9806\n",
      "Epoch 27/500\n",
      "8s - loss: 2.3826 - act_output_loss: 1.3803 - time_output_loss: 0.9998 - val_loss: 2.4074 - val_act_output_loss: 1.4301 - val_time_output_loss: 0.9744\n",
      "Epoch 28/500\n",
      "9s - loss: 2.3833 - act_output_loss: 1.3844 - time_output_loss: 0.9959 - val_loss: 2.4260 - val_act_output_loss: 1.4360 - val_time_output_loss: 0.9852\n",
      "Epoch 29/500\n",
      "7s - loss: 2.3784 - act_output_loss: 1.3812 - time_output_loss: 0.9936 - val_loss: 2.3997 - val_act_output_loss: 1.4287 - val_time_output_loss: 0.9669\n",
      "Epoch 30/500\n",
      "8s - loss: 2.3779 - act_output_loss: 1.3805 - time_output_loss: 0.9942 - val_loss: 2.4101 - val_act_output_loss: 1.4273 - val_time_output_loss: 0.9800\n",
      "Epoch 31/500\n",
      "7s - loss: 2.3719 - act_output_loss: 1.3768 - time_output_loss: 0.9920 - val_loss: 2.4112 - val_act_output_loss: 1.4338 - val_time_output_loss: 0.9733\n",
      "Epoch 32/500\n",
      "7s - loss: 2.3785 - act_output_loss: 1.3838 - time_output_loss: 0.9911 - val_loss: 2.4415 - val_act_output_loss: 1.4278 - val_time_output_loss: 1.0109\n",
      "Epoch 33/500\n",
      "8s - loss: 2.3753 - act_output_loss: 1.3804 - time_output_loss: 0.9914 - val_loss: 2.4114 - val_act_output_loss: 1.4367 - val_time_output_loss: 0.9709\n",
      "Epoch 34/500\n",
      "7s - loss: 2.3695 - act_output_loss: 1.3745 - time_output_loss: 0.9923 - val_loss: 2.4197 - val_act_output_loss: 1.4292 - val_time_output_loss: 0.9872\n",
      "Epoch 35/500\n",
      "8s - loss: 2.3664 - act_output_loss: 1.3760 - time_output_loss: 0.9868 - val_loss: 2.3947 - val_act_output_loss: 1.4257 - val_time_output_loss: 0.9651\n",
      "Epoch 36/500\n",
      "8s - loss: 2.3689 - act_output_loss: 1.3773 - time_output_loss: 0.9882 - val_loss: 2.3963 - val_act_output_loss: 1.4265 - val_time_output_loss: 0.9669\n",
      "Epoch 37/500\n",
      "7s - loss: 2.3660 - act_output_loss: 1.3773 - time_output_loss: 0.9855 - val_loss: 2.3941 - val_act_output_loss: 1.4254 - val_time_output_loss: 0.9658\n",
      "Epoch 38/500\n",
      "11s - loss: 2.3691 - act_output_loss: 1.3765 - time_output_loss: 0.9890 - val_loss: 2.3910 - val_act_output_loss: 1.4237 - val_time_output_loss: 0.9634\n",
      "Epoch 39/500\n",
      "10s - loss: 2.3561 - act_output_loss: 1.3712 - time_output_loss: 0.9813 - val_loss: 2.4119 - val_act_output_loss: 1.4361 - val_time_output_loss: 0.9720\n",
      "Epoch 40/500\n",
      "8s - loss: 2.3568 - act_output_loss: 1.3720 - time_output_loss: 0.9813 - val_loss: 2.3968 - val_act_output_loss: 1.4347 - val_time_output_loss: 0.9588\n",
      "Epoch 41/500\n",
      "9s - loss: 2.3614 - act_output_loss: 1.3768 - time_output_loss: 0.9807 - val_loss: 2.3997 - val_act_output_loss: 1.4236 - val_time_output_loss: 0.9733\n",
      "Epoch 42/500\n",
      "8s - loss: 2.3568 - act_output_loss: 1.3725 - time_output_loss: 0.9811 - val_loss: 2.3967 - val_act_output_loss: 1.4333 - val_time_output_loss: 0.9598\n",
      "Epoch 43/500\n",
      "7s - loss: 2.3632 - act_output_loss: 1.3796 - time_output_loss: 0.9803 - val_loss: 2.3989 - val_act_output_loss: 1.4290 - val_time_output_loss: 0.9673\n",
      "Epoch 44/500\n",
      "9s - loss: 2.3166 - act_output_loss: 1.3342 - time_output_loss: 0.9797 - val_loss: 2.2295 - val_act_output_loss: 1.2528 - val_time_output_loss: 0.9732\n",
      "Epoch 45/500\n",
      "8s - loss: 2.2047 - act_output_loss: 1.2172 - time_output_loss: 0.9842 - val_loss: 2.2631 - val_act_output_loss: 1.2957 - val_time_output_loss: 0.9646\n",
      "Epoch 46/500\n",
      "7s - loss: 2.1380 - act_output_loss: 1.1573 - time_output_loss: 0.9778 - val_loss: 2.2257 - val_act_output_loss: 1.2457 - val_time_output_loss: 0.9770\n",
      "Epoch 47/500\n",
      "8s - loss: 2.1312 - act_output_loss: 1.1442 - time_output_loss: 0.9835 - val_loss: 2.3012 - val_act_output_loss: 1.3369 - val_time_output_loss: 0.9605\n",
      "Epoch 48/500\n",
      "7s - loss: 2.1112 - act_output_loss: 1.1280 - time_output_loss: 0.9801 - val_loss: 2.2672 - val_act_output_loss: 1.2933 - val_time_output_loss: 0.9717\n",
      "Epoch 49/500\n",
      "10s - loss: 2.1110 - act_output_loss: 1.1270 - time_output_loss: 0.9802 - val_loss: 2.0337 - val_act_output_loss: 1.0649 - val_time_output_loss: 0.9652\n",
      "Epoch 50/500\n",
      "8s - loss: 2.0983 - act_output_loss: 1.1191 - time_output_loss: 0.9758 - val_loss: 2.1136 - val_act_output_loss: 1.1509 - val_time_output_loss: 0.9596\n",
      "Epoch 51/500\n",
      "7s - loss: 2.1049 - act_output_loss: 1.1195 - time_output_loss: 0.9825 - val_loss: 2.0185 - val_act_output_loss: 1.0418 - val_time_output_loss: 0.9741\n",
      "Epoch 52/500\n",
      "8s - loss: 2.0646 - act_output_loss: 1.0867 - time_output_loss: 0.9750 - val_loss: 1.9471 - val_act_output_loss: 0.9826 - val_time_output_loss: 0.9618\n",
      "Epoch 53/500\n",
      "8s - loss: 2.0224 - act_output_loss: 1.0381 - time_output_loss: 0.9813 - val_loss: 2.0226 - val_act_output_loss: 1.0549 - val_time_output_loss: 0.9650\n",
      "Epoch 54/500\n",
      "7s - loss: 1.9535 - act_output_loss: 0.9714 - time_output_loss: 0.9786 - val_loss: 1.9147 - val_act_output_loss: 0.9319 - val_time_output_loss: 0.9792\n",
      "Epoch 55/500\n",
      "8s - loss: 1.8793 - act_output_loss: 0.8926 - time_output_loss: 0.9840 - val_loss: 1.8351 - val_act_output_loss: 0.8682 - val_time_output_loss: 0.9610\n",
      "Epoch 56/500\n",
      "8s - loss: 1.8544 - act_output_loss: 0.8720 - time_output_loss: 0.9795 - val_loss: 1.8280 - val_act_output_loss: 0.8658 - val_time_output_loss: 0.9592\n",
      "Epoch 57/500\n",
      "8s - loss: 1.8426 - act_output_loss: 0.8610 - time_output_loss: 0.9787 - val_loss: 1.8008 - val_act_output_loss: 0.8343 - val_time_output_loss: 0.9638\n",
      "Epoch 58/500\n",
      "8s - loss: 1.8396 - act_output_loss: 0.8581 - time_output_loss: 0.9786 - val_loss: 1.7924 - val_act_output_loss: 0.8289 - val_time_output_loss: 0.9608\n",
      "Epoch 59/500\n",
      "8s - loss: 1.8252 - act_output_loss: 0.8482 - time_output_loss: 0.9738 - val_loss: 1.8167 - val_act_output_loss: 0.8510 - val_time_output_loss: 0.9632\n",
      "Epoch 60/500\n",
      "7s - loss: 1.8245 - act_output_loss: 0.8453 - time_output_loss: 0.9767 - val_loss: 1.7952 - val_act_output_loss: 0.8283 - val_time_output_loss: 0.9638\n",
      "Epoch 61/500\n",
      "7s - loss: 1.8211 - act_output_loss: 0.8431 - time_output_loss: 0.9753 - val_loss: 1.7765 - val_act_output_loss: 0.8167 - val_time_output_loss: 0.9572\n",
      "Epoch 62/500\n",
      "8s - loss: 1.8157 - act_output_loss: 0.8387 - time_output_loss: 0.9746 - val_loss: 1.7976 - val_act_output_loss: 0.8381 - val_time_output_loss: 0.9570\n",
      "Epoch 63/500\n",
      "8s - loss: 1.8187 - act_output_loss: 0.8404 - time_output_loss: 0.9753 - val_loss: 1.9150 - val_act_output_loss: 0.9410 - val_time_output_loss: 0.9702\n",
      "Epoch 64/500\n",
      "8s - loss: 1.8178 - act_output_loss: 0.8396 - time_output_loss: 0.9753 - val_loss: 1.8057 - val_act_output_loss: 0.8403 - val_time_output_loss: 0.9628\n",
      "Epoch 65/500\n",
      "7s - loss: 1.8172 - act_output_loss: 0.8374 - time_output_loss: 0.9759 - val_loss: 1.7748 - val_act_output_loss: 0.8144 - val_time_output_loss: 0.9575\n",
      "Epoch 66/500\n",
      "8s - loss: 1.8153 - act_output_loss: 0.8389 - time_output_loss: 0.9732 - val_loss: 1.8059 - val_act_output_loss: 0.8400 - val_time_output_loss: 0.9637\n",
      "Epoch 67/500\n",
      "7s - loss: 1.8083 - act_output_loss: 0.8331 - time_output_loss: 0.9729 - val_loss: 1.8183 - val_act_output_loss: 0.8498 - val_time_output_loss: 0.9659\n",
      "Epoch 68/500\n",
      "7s - loss: 1.8012 - act_output_loss: 0.8258 - time_output_loss: 0.9730 - val_loss: 1.8041 - val_act_output_loss: 0.8395 - val_time_output_loss: 0.9614\n",
      "Epoch 69/500\n",
      "7s - loss: 1.8211 - act_output_loss: 0.8423 - time_output_loss: 0.9756 - val_loss: 1.8400 - val_act_output_loss: 0.8678 - val_time_output_loss: 0.9701\n",
      "Epoch 70/500\n",
      "7s - loss: 1.8001 - act_output_loss: 0.8274 - time_output_loss: 0.9700 - val_loss: 1.8124 - val_act_output_loss: 0.8496 - val_time_output_loss: 0.9605\n",
      "Epoch 71/500\n",
      "7s - loss: 1.8067 - act_output_loss: 0.8377 - time_output_loss: 0.9664 - val_loss: 1.7881 - val_act_output_loss: 0.8269 - val_time_output_loss: 0.9587\n",
      "Epoch 72/500\n",
      "7s - loss: 1.8041 - act_output_loss: 0.8312 - time_output_loss: 0.9704 - val_loss: 1.7919 - val_act_output_loss: 0.8288 - val_time_output_loss: 0.9605\n",
      "Epoch 73/500\n",
      "7s - loss: 1.7975 - act_output_loss: 0.8273 - time_output_loss: 0.9674 - val_loss: 1.7900 - val_act_output_loss: 0.8320 - val_time_output_loss: 0.9550\n",
      "Epoch 74/500\n",
      "7s - loss: 1.7972 - act_output_loss: 0.8291 - time_output_loss: 0.9654 - val_loss: 1.7712 - val_act_output_loss: 0.8104 - val_time_output_loss: 0.9575\n",
      "Epoch 75/500\n",
      "8s - loss: 1.8016 - act_output_loss: 0.8228 - time_output_loss: 0.9760 - val_loss: 1.8489 - val_act_output_loss: 0.8801 - val_time_output_loss: 0.9667\n",
      "Epoch 76/500\n",
      "7s - loss: 1.7960 - act_output_loss: 0.8228 - time_output_loss: 0.9703 - val_loss: 1.7861 - val_act_output_loss: 0.8150 - val_time_output_loss: 0.9684\n",
      "Epoch 77/500\n",
      "7s - loss: 1.8097 - act_output_loss: 0.8328 - time_output_loss: 0.9742 - val_loss: 1.8503 - val_act_output_loss: 0.8707 - val_time_output_loss: 0.9777\n",
      "Epoch 78/500\n",
      "7s - loss: 1.7920 - act_output_loss: 0.8213 - time_output_loss: 0.9672 - val_loss: 1.8025 - val_act_output_loss: 0.8355 - val_time_output_loss: 0.9598\n",
      "Epoch 79/500\n",
      "8s - loss: 1.7944 - act_output_loss: 0.8222 - time_output_loss: 0.9692 - val_loss: 1.7796 - val_act_output_loss: 0.8113 - val_time_output_loss: 0.9661\n",
      "Epoch 80/500\n",
      "8s - loss: 1.7993 - act_output_loss: 0.8313 - time_output_loss: 0.9656 - val_loss: 1.7812 - val_act_output_loss: 0.8178 - val_time_output_loss: 0.9609\n",
      "Epoch 81/500\n",
      "9s - loss: 1.7884 - act_output_loss: 0.8208 - time_output_loss: 0.9651 - val_loss: 1.7822 - val_act_output_loss: 0.8151 - val_time_output_loss: 0.9648\n",
      "Epoch 82/500\n",
      "7s - loss: 1.7884 - act_output_loss: 0.8157 - time_output_loss: 0.9701 - val_loss: 1.7889 - val_act_output_loss: 0.8153 - val_time_output_loss: 0.9716\n",
      "Epoch 83/500\n",
      "7s - loss: 1.7925 - act_output_loss: 0.8231 - time_output_loss: 0.9664 - val_loss: 1.8202 - val_act_output_loss: 0.8456 - val_time_output_loss: 0.9695\n",
      "Epoch 84/500\n",
      "7s - loss: 1.7990 - act_output_loss: 0.8208 - time_output_loss: 0.9742 - val_loss: 1.7884 - val_act_output_loss: 0.8214 - val_time_output_loss: 0.9649\n",
      "Epoch 85/500\n",
      "7s - loss: 1.7838 - act_output_loss: 0.8176 - time_output_loss: 0.9627 - val_loss: 1.7870 - val_act_output_loss: 0.8257 - val_time_output_loss: 0.9580\n",
      "Epoch 86/500\n",
      "7s - loss: 1.7730 - act_output_loss: 0.8103 - time_output_loss: 0.9601 - val_loss: 1.7863 - val_act_output_loss: 0.8263 - val_time_output_loss: 0.9576\n",
      "Epoch 87/500\n",
      "10s - loss: 1.7808 - act_output_loss: 0.8144 - time_output_loss: 0.9641 - val_loss: 1.7743 - val_act_output_loss: 0.8115 - val_time_output_loss: 0.9606\n",
      "Epoch 88/500\n",
      "12s - loss: 1.7720 - act_output_loss: 0.8108 - time_output_loss: 0.9592 - val_loss: 1.7701 - val_act_output_loss: 0.8100 - val_time_output_loss: 0.9576\n",
      "Epoch 89/500\n",
      "8s - loss: 1.7826 - act_output_loss: 0.8150 - time_output_loss: 0.9655 - val_loss: 1.7767 - val_act_output_loss: 0.8158 - val_time_output_loss: 0.9590\n",
      "Epoch 90/500\n",
      "8s - loss: 1.7724 - act_output_loss: 0.8113 - time_output_loss: 0.9590 - val_loss: 1.7788 - val_act_output_loss: 0.8154 - val_time_output_loss: 0.9614\n",
      "Epoch 91/500\n",
      "8s - loss: 1.7690 - act_output_loss: 0.8094 - time_output_loss: 0.9576 - val_loss: 1.7809 - val_act_output_loss: 0.8161 - val_time_output_loss: 0.9627\n",
      "Epoch 92/500\n",
      "8s - loss: 1.7755 - act_output_loss: 0.8102 - time_output_loss: 0.9630 - val_loss: 1.8128 - val_act_output_loss: 0.8480 - val_time_output_loss: 0.9619\n",
      "Epoch 93/500\n",
      "8s - loss: 1.7831 - act_output_loss: 0.8148 - time_output_loss: 0.9659 - val_loss: 1.7779 - val_act_output_loss: 0.8171 - val_time_output_loss: 0.9588\n",
      "Epoch 94/500\n",
      "8s - loss: 1.7744 - act_output_loss: 0.8097 - time_output_loss: 0.9625 - val_loss: 1.7724 - val_act_output_loss: 0.8134 - val_time_output_loss: 0.9572\n",
      "Epoch 95/500\n",
      "8s - loss: 1.7730 - act_output_loss: 0.8133 - time_output_loss: 0.9577 - val_loss: 1.7833 - val_act_output_loss: 0.8188 - val_time_output_loss: 0.9622\n",
      "Epoch 96/500\n",
      "8s - loss: 1.7754 - act_output_loss: 0.8123 - time_output_loss: 0.9610 - val_loss: 1.7950 - val_act_output_loss: 0.8332 - val_time_output_loss: 0.9596\n",
      "Epoch 97/500\n",
      "8s - loss: 1.7761 - act_output_loss: 0.8146 - time_output_loss: 0.9593 - val_loss: 1.7785 - val_act_output_loss: 0.8197 - val_time_output_loss: 0.9554\n",
      "Epoch 98/500\n",
      "8s - loss: 1.7715 - act_output_loss: 0.8132 - time_output_loss: 0.9559 - val_loss: 1.8334 - val_act_output_loss: 0.8614 - val_time_output_loss: 0.9702\n",
      "Epoch 99/500\n",
      "8s - loss: 1.7749 - act_output_loss: 0.8120 - time_output_loss: 0.9610 - val_loss: 1.7922 - val_act_output_loss: 0.8217 - val_time_output_loss: 0.9687\n",
      "Epoch 100/500\n",
      "8s - loss: 1.7655 - act_output_loss: 0.8043 - time_output_loss: 0.9593 - val_loss: 1.7675 - val_act_output_loss: 0.8114 - val_time_output_loss: 0.9541\n",
      "Epoch 101/500\n",
      "8s - loss: 1.7641 - act_output_loss: 0.8033 - time_output_loss: 0.9590 - val_loss: 1.7703 - val_act_output_loss: 0.8127 - val_time_output_loss: 0.9558\n",
      "Epoch 102/500\n",
      "8s - loss: 1.7692 - act_output_loss: 0.8064 - time_output_loss: 0.9610 - val_loss: 1.7801 - val_act_output_loss: 0.8201 - val_time_output_loss: 0.9583\n",
      "Epoch 103/500\n",
      "7s - loss: 1.7663 - act_output_loss: 0.8074 - time_output_loss: 0.9571 - val_loss: 1.7740 - val_act_output_loss: 0.8109 - val_time_output_loss: 0.9615\n",
      "Epoch 104/500\n",
      "8s - loss: 1.7658 - act_output_loss: 0.8063 - time_output_loss: 0.9577 - val_loss: 1.7711 - val_act_output_loss: 0.8122 - val_time_output_loss: 0.9573\n",
      "Epoch 105/500\n",
      "7s - loss: 1.7648 - act_output_loss: 0.8083 - time_output_loss: 0.9547 - val_loss: 1.7798 - val_act_output_loss: 0.8234 - val_time_output_loss: 0.9547\n",
      "Epoch 106/500\n",
      "8s - loss: 1.7655 - act_output_loss: 0.8038 - time_output_loss: 0.9600 - val_loss: 1.7697 - val_act_output_loss: 0.8118 - val_time_output_loss: 0.9563\n",
      "Epoch 107/500\n",
      "8s - loss: 1.7614 - act_output_loss: 0.8044 - time_output_loss: 0.9553 - val_loss: 1.7730 - val_act_output_loss: 0.8096 - val_time_output_loss: 0.9618\n",
      "Epoch 108/500\n",
      "8s - loss: 1.7623 - act_output_loss: 0.8016 - time_output_loss: 0.9590 - val_loss: 1.7767 - val_act_output_loss: 0.8161 - val_time_output_loss: 0.9589\n",
      "Epoch 109/500\n",
      "8s - loss: 1.7586 - act_output_loss: 0.7983 - time_output_loss: 0.9586 - val_loss: 1.7735 - val_act_output_loss: 0.8132 - val_time_output_loss: 0.9587\n",
      "Epoch 110/500\n",
      "8s - loss: 1.7623 - act_output_loss: 0.8044 - time_output_loss: 0.9563 - val_loss: 1.7843 - val_act_output_loss: 0.8209 - val_time_output_loss: 0.9616\n",
      "Epoch 111/500\n",
      "8s - loss: 1.7657 - act_output_loss: 0.8070 - time_output_loss: 0.9570 - val_loss: 1.7716 - val_act_output_loss: 0.8096 - val_time_output_loss: 0.9605\n",
      "Epoch 112/500\n",
      "8s - loss: 1.7627 - act_output_loss: 0.7998 - time_output_loss: 0.9614 - val_loss: 1.7674 - val_act_output_loss: 0.8097 - val_time_output_loss: 0.9562\n",
      "Epoch 113/500\n",
      "8s - loss: 1.7646 - act_output_loss: 0.8058 - time_output_loss: 0.9574 - val_loss: 1.7686 - val_act_output_loss: 0.8096 - val_time_output_loss: 0.9576\n",
      "Epoch 114/500\n",
      "8s - loss: 1.7653 - act_output_loss: 0.8047 - time_output_loss: 0.9591 - val_loss: 1.7731 - val_act_output_loss: 0.8120 - val_time_output_loss: 0.9596\n",
      "Epoch 115/500\n",
      "8s - loss: 1.7594 - act_output_loss: 0.8073 - time_output_loss: 0.9505 - val_loss: 1.7660 - val_act_output_loss: 0.8094 - val_time_output_loss: 0.9550\n",
      "Epoch 116/500\n",
      "8s - loss: 1.7687 - act_output_loss: 0.8039 - time_output_loss: 0.9633 - val_loss: 1.7668 - val_act_output_loss: 0.8095 - val_time_output_loss: 0.9559\n",
      "Epoch 117/500\n",
      "9s - loss: 1.7687 - act_output_loss: 0.8075 - time_output_loss: 0.9598 - val_loss: 1.7704 - val_act_output_loss: 0.8125 - val_time_output_loss: 0.9565\n",
      "Epoch 118/500\n",
      "8s - loss: 1.7630 - act_output_loss: 0.7998 - time_output_loss: 0.9618 - val_loss: 1.7672 - val_act_output_loss: 0.8088 - val_time_output_loss: 0.9571\n",
      "Epoch 119/500\n",
      "8s - loss: 1.7651 - act_output_loss: 0.8044 - time_output_loss: 0.9593 - val_loss: 1.7704 - val_act_output_loss: 0.8121 - val_time_output_loss: 0.9569\n",
      "Epoch 120/500\n",
      "7s - loss: 1.7613 - act_output_loss: 0.8018 - time_output_loss: 0.9580 - val_loss: 1.7686 - val_act_output_loss: 0.8114 - val_time_output_loss: 0.9558\n",
      "Epoch 121/500\n",
      "8s - loss: 1.7578 - act_output_loss: 0.8036 - time_output_loss: 0.9527 - val_loss: 1.7709 - val_act_output_loss: 0.8126 - val_time_output_loss: 0.9567\n",
      "Epoch 122/500\n",
      "8s - loss: 1.7606 - act_output_loss: 0.8040 - time_output_loss: 0.9549 - val_loss: 1.7778 - val_act_output_loss: 0.8159 - val_time_output_loss: 0.9605\n",
      "Epoch 123/500\n",
      "8s - loss: 1.7668 - act_output_loss: 0.8030 - time_output_loss: 0.9624 - val_loss: 1.7706 - val_act_output_loss: 0.8100 - val_time_output_loss: 0.9592\n",
      "Epoch 124/500\n",
      "8s - loss: 1.7571 - act_output_loss: 0.7996 - time_output_loss: 0.9560 - val_loss: 1.7710 - val_act_output_loss: 0.8118 - val_time_output_loss: 0.9577\n",
      "Epoch 125/500\n",
      "8s - loss: 1.7558 - act_output_loss: 0.7977 - time_output_loss: 0.9567 - val_loss: 1.7705 - val_act_output_loss: 0.8118 - val_time_output_loss: 0.9573\n",
      "Epoch 126/500\n",
      "8s - loss: 1.7608 - act_output_loss: 0.8005 - time_output_loss: 0.9589 - val_loss: 1.7715 - val_act_output_loss: 0.8126 - val_time_output_loss: 0.9575\n",
      "Epoch 127/500\n",
      "8s - loss: 1.7655 - act_output_loss: 0.8003 - time_output_loss: 0.9638 - val_loss: 1.7706 - val_act_output_loss: 0.8121 - val_time_output_loss: 0.9571\n",
      "Epoch 128/500\n",
      "8s - loss: 1.7623 - act_output_loss: 0.8034 - time_output_loss: 0.9576 - val_loss: 1.7703 - val_act_output_loss: 0.8116 - val_time_output_loss: 0.9573\n",
      "Epoch 129/500\n",
      "8s - loss: 1.7592 - act_output_loss: 0.8018 - time_output_loss: 0.9561 - val_loss: 1.7686 - val_act_output_loss: 0.8112 - val_time_output_loss: 0.9560\n",
      "Epoch 130/500\n",
      "8s - loss: 1.7570 - act_output_loss: 0.8000 - time_output_loss: 0.9555 - val_loss: 1.7733 - val_act_output_loss: 0.8159 - val_time_output_loss: 0.9560\n",
      "Epoch 131/500\n",
      "8s - loss: 1.7463 - act_output_loss: 0.7982 - time_output_loss: 0.9467 - val_loss: 1.7712 - val_act_output_loss: 0.8132 - val_time_output_loss: 0.9566\n",
      "Epoch 132/500\n",
      "7s - loss: 1.7573 - act_output_loss: 0.7966 - time_output_loss: 0.9592 - val_loss: 1.7682 - val_act_output_loss: 0.8120 - val_time_output_loss: 0.9547\n",
      "Epoch 133/500\n",
      "8s - loss: 1.7561 - act_output_loss: 0.7959 - time_output_loss: 0.9587 - val_loss: 1.7726 - val_act_output_loss: 0.8148 - val_time_output_loss: 0.9564\n",
      "Epoch 134/500\n",
      "8s - loss: 1.7554 - act_output_loss: 0.7950 - time_output_loss: 0.9590 - val_loss: 1.7722 - val_act_output_loss: 0.8141 - val_time_output_loss: 0.9567\n",
      "Epoch 135/500\n",
      "7s - loss: 1.7617 - act_output_loss: 0.7989 - time_output_loss: 0.9613 - val_loss: 1.7710 - val_act_output_loss: 0.8119 - val_time_output_loss: 0.9577\n",
      "Epoch 136/500\n",
      "8s - loss: 1.7623 - act_output_loss: 0.8034 - time_output_loss: 0.9574 - val_loss: 1.7697 - val_act_output_loss: 0.8116 - val_time_output_loss: 0.9567\n",
      "Epoch 137/500\n",
      "8s - loss: 1.7530 - act_output_loss: 0.7957 - time_output_loss: 0.9559 - val_loss: 1.7685 - val_act_output_loss: 0.8106 - val_time_output_loss: 0.9565\n",
      "Epoch 138/500\n",
      "8s - loss: 1.7548 - act_output_loss: 0.7948 - time_output_loss: 0.9586 - val_loss: 1.7688 - val_act_output_loss: 0.8110 - val_time_output_loss: 0.9564\n",
      "Epoch 139/500\n",
      "8s - loss: 1.7494 - act_output_loss: 0.7948 - time_output_loss: 0.9532 - val_loss: 1.7702 - val_act_output_loss: 0.8121 - val_time_output_loss: 0.9567\n",
      "Epoch 140/500\n",
      "8s - loss: 1.7631 - act_output_loss: 0.8005 - time_output_loss: 0.9612 - val_loss: 1.7717 - val_act_output_loss: 0.8134 - val_time_output_loss: 0.9570\n",
      "Epoch 141/500\n",
      "8s - loss: 1.7509 - act_output_loss: 0.7976 - time_output_loss: 0.9519 - val_loss: 1.7700 - val_act_output_loss: 0.8116 - val_time_output_loss: 0.9570\n",
      "Epoch 142/500\n",
      "8s - loss: 1.7556 - act_output_loss: 0.8022 - time_output_loss: 0.9520 - val_loss: 1.7696 - val_act_output_loss: 0.8117 - val_time_output_loss: 0.9565\n",
      "Epoch 143/500\n",
      "8s - loss: 1.7523 - act_output_loss: 0.7917 - time_output_loss: 0.9593 - val_loss: 1.7711 - val_act_output_loss: 0.8113 - val_time_output_loss: 0.9584\n",
      "Epoch 144/500\n",
      "8s - loss: 1.7457 - act_output_loss: 0.7968 - time_output_loss: 0.9475 - val_loss: 1.7695 - val_act_output_loss: 0.8115 - val_time_output_loss: 0.9565\n",
      "Epoch 145/500\n",
      "8s - loss: 1.7532 - act_output_loss: 0.7995 - time_output_loss: 0.9524 - val_loss: 1.7698 - val_act_output_loss: 0.8124 - val_time_output_loss: 0.9560\n",
      "Epoch 146/500\n",
      "8s - loss: 1.7579 - act_output_loss: 0.8014 - time_output_loss: 0.9551 - val_loss: 1.7711 - val_act_output_loss: 0.8116 - val_time_output_loss: 0.9581\n",
      "Epoch 147/500\n",
      "12s - loss: 1.7505 - act_output_loss: 0.7937 - time_output_loss: 0.9555 - val_loss: 1.7689 - val_act_output_loss: 0.8109 - val_time_output_loss: 0.9566\n",
      "Epoch 148/500\n",
      "9s - loss: 1.7605 - act_output_loss: 0.8007 - time_output_loss: 0.9583 - val_loss: 1.7693 - val_act_output_loss: 0.8116 - val_time_output_loss: 0.9563\n",
      "Epoch 149/500\n",
      "7s - loss: 1.7575 - act_output_loss: 0.8004 - time_output_loss: 0.9557 - val_loss: 1.7692 - val_act_output_loss: 0.8115 - val_time_output_loss: 0.9563\n",
      "Epoch 150/500\n",
      "8s - loss: 1.7489 - act_output_loss: 0.7939 - time_output_loss: 0.9536 - val_loss: 1.7696 - val_act_output_loss: 0.8121 - val_time_output_loss: 0.9561\n",
      "Epoch 151/500\n",
      "8s - loss: 1.7564 - act_output_loss: 0.8020 - time_output_loss: 0.9530 - val_loss: 1.7698 - val_act_output_loss: 0.8120 - val_time_output_loss: 0.9564\n",
      "Epoch 152/500\n",
      "7s - loss: 1.7530 - act_output_loss: 0.7971 - time_output_loss: 0.9545 - val_loss: 1.7709 - val_act_output_loss: 0.8124 - val_time_output_loss: 0.9571\n",
      "Epoch 153/500\n",
      "9s - loss: 1.7531 - act_output_loss: 0.7976 - time_output_loss: 0.9541 - val_loss: 1.7708 - val_act_output_loss: 0.8124 - val_time_output_loss: 0.9570\n",
      "Epoch 154/500\n",
      "7s - loss: 1.7553 - act_output_loss: 0.8005 - time_output_loss: 0.9534 - val_loss: 1.7707 - val_act_output_loss: 0.8124 - val_time_output_loss: 0.9569\n",
      "Epoch 155/500\n",
      "7s - loss: 1.7619 - act_output_loss: 0.8015 - time_output_loss: 0.9590 - val_loss: 1.7703 - val_act_output_loss: 0.8121 - val_time_output_loss: 0.9568\n",
      "Epoch 156/500\n",
      "7s - loss: 1.7638 - act_output_loss: 0.7976 - time_output_loss: 0.9649 - val_loss: 1.7695 - val_act_output_loss: 0.8113 - val_time_output_loss: 0.9569\n",
      "Epoch 157/500\n",
      "7s - loss: 1.7532 - act_output_loss: 0.7965 - time_output_loss: 0.9553 - val_loss: 1.7701 - val_act_output_loss: 0.8122 - val_time_output_loss: 0.9566\n",
      "Epoch 158/500\n",
      "7s - loss: 1.7545 - act_output_loss: 0.7953 - time_output_loss: 0.9579 - val_loss: 1.7699 - val_act_output_loss: 0.8121 - val_time_output_loss: 0.9565\n",
      "Epoch 159/500\n",
      "7s - loss: 1.7538 - act_output_loss: 0.8028 - time_output_loss: 0.9496 - val_loss: 1.7700 - val_act_output_loss: 0.8122 - val_time_output_loss: 0.9564\n",
      "Epoch 160/500\n",
      "7s - loss: 1.7500 - act_output_loss: 0.7944 - time_output_loss: 0.9543 - val_loss: 1.7688 - val_act_output_loss: 0.8110 - val_time_output_loss: 0.9564\n",
      "Epoch 161/500\n",
      "7s - loss: 1.7613 - act_output_loss: 0.8044 - time_output_loss: 0.9556 - val_loss: 1.7712 - val_act_output_loss: 0.8129 - val_time_output_loss: 0.9569\n",
      "Epoch 162/500\n",
      "8s - loss: 1.7503 - act_output_loss: 0.7959 - time_output_loss: 0.9530 - val_loss: 1.7713 - val_act_output_loss: 0.8130 - val_time_output_loss: 0.9569\n",
      "Epoch 163/500\n",
      "7s - loss: 1.7580 - act_output_loss: 0.8002 - time_output_loss: 0.9564 - val_loss: 1.7690 - val_act_output_loss: 0.8111 - val_time_output_loss: 0.9565\n",
      "Epoch 164/500\n",
      "7s - loss: 1.7578 - act_output_loss: 0.7983 - time_output_loss: 0.9582 - val_loss: 1.7696 - val_act_output_loss: 0.8119 - val_time_output_loss: 0.9563\n",
      "Epoch 165/500\n",
      "8s - loss: 1.7496 - act_output_loss: 0.7934 - time_output_loss: 0.9549 - val_loss: 1.7694 - val_act_output_loss: 0.8117 - val_time_output_loss: 0.9564\n",
      "Epoch 166/500\n",
      "10s - loss: 1.7592 - act_output_loss: 0.7994 - time_output_loss: 0.9584 - val_loss: 1.7690 - val_act_output_loss: 0.8112 - val_time_output_loss: 0.9565\n",
      "Epoch 167/500\n",
      "9s - loss: 1.7516 - act_output_loss: 0.7939 - time_output_loss: 0.9563 - val_loss: 1.7693 - val_act_output_loss: 0.8115 - val_time_output_loss: 0.9565\n",
      "Epoch 168/500\n",
      "10s - loss: 1.7605 - act_output_loss: 0.8026 - time_output_loss: 0.9565 - val_loss: 1.7699 - val_act_output_loss: 0.8115 - val_time_output_loss: 0.9570\n",
      "Epoch 169/500\n",
      "12s - loss: 1.7506 - act_output_loss: 0.7958 - time_output_loss: 0.9535 - val_loss: 1.7690 - val_act_output_loss: 0.8112 - val_time_output_loss: 0.9565\n",
      "Epoch 170/500\n",
      "11s - loss: 1.7562 - act_output_loss: 0.8019 - time_output_loss: 0.9530 - val_loss: 1.7698 - val_act_output_loss: 0.8117 - val_time_output_loss: 0.9568\n",
      "Epoch 171/500\n",
      "7s - loss: 1.7578 - act_output_loss: 0.7978 - time_output_loss: 0.9587 - val_loss: 1.7706 - val_act_output_loss: 0.8125 - val_time_output_loss: 0.9567\n",
      "Epoch 172/500\n",
      "12s - loss: 1.7457 - act_output_loss: 0.7950 - time_output_loss: 0.9494 - val_loss: 1.7695 - val_act_output_loss: 0.8114 - val_time_output_loss: 0.9567\n",
      "Epoch 173/500\n",
      "16s - loss: 1.7565 - act_output_loss: 0.8006 - time_output_loss: 0.9546 - val_loss: 1.7700 - val_act_output_loss: 0.8119 - val_time_output_loss: 0.9567\n",
      "Epoch 174/500\n",
      "9s - loss: 1.7500 - act_output_loss: 0.7933 - time_output_loss: 0.9553 - val_loss: 1.7696 - val_act_output_loss: 0.8118 - val_time_output_loss: 0.9565\n",
      "Epoch 175/500\n",
      "10s - loss: 1.7471 - act_output_loss: 0.7958 - time_output_loss: 0.9499 - val_loss: 1.7692 - val_act_output_loss: 0.8113 - val_time_output_loss: 0.9565\n",
      "Epoch 176/500\n",
      "8s - loss: 1.7501 - act_output_loss: 0.7943 - time_output_loss: 0.9544 - val_loss: 1.7690 - val_act_output_loss: 0.8113 - val_time_output_loss: 0.9563\n",
      "Epoch 177/500\n",
      "9s - loss: 1.7510 - act_output_loss: 0.7918 - time_output_loss: 0.9578 - val_loss: 1.7701 - val_act_output_loss: 0.8125 - val_time_output_loss: 0.9562\n",
      "Epoch 178/500\n",
      "8s - loss: 1.7543 - act_output_loss: 0.7978 - time_output_loss: 0.9552 - val_loss: 1.7694 - val_act_output_loss: 0.8118 - val_time_output_loss: 0.9562\n",
      "Epoch 179/500\n",
      "8s - loss: 1.7610 - act_output_loss: 0.8012 - time_output_loss: 0.9585 - val_loss: 1.7701 - val_act_output_loss: 0.8125 - val_time_output_loss: 0.9562\n",
      "Epoch 180/500\n",
      "8s - loss: 1.7587 - act_output_loss: 0.7994 - time_output_loss: 0.9579 - val_loss: 1.7707 - val_act_output_loss: 0.8125 - val_time_output_loss: 0.9568\n",
      "Epoch 181/500\n",
      "14s - loss: 1.7481 - act_output_loss: 0.7907 - time_output_loss: 0.9560 - val_loss: 1.7699 - val_act_output_loss: 0.8121 - val_time_output_loss: 0.9565\n",
      "Epoch 182/500\n",
      "8s - loss: 1.7536 - act_output_loss: 0.7985 - time_output_loss: 0.9537 - val_loss: 1.7695 - val_act_output_loss: 0.8115 - val_time_output_loss: 0.9567\n",
      "Epoch 183/500\n",
      "7s - loss: 1.7532 - act_output_loss: 0.7973 - time_output_loss: 0.9545 - val_loss: 1.7688 - val_act_output_loss: 0.8108 - val_time_output_loss: 0.9567\n",
      "Epoch 184/500\n",
      "7s - loss: 1.7559 - act_output_loss: 0.7951 - time_output_loss: 0.9594 - val_loss: 1.7701 - val_act_output_loss: 0.8120 - val_time_output_loss: 0.9567\n",
      "Epoch 185/500\n",
      "9s - loss: 1.7481 - act_output_loss: 0.7969 - time_output_loss: 0.9499 - val_loss: 1.7698 - val_act_output_loss: 0.8118 - val_time_output_loss: 0.9566\n",
      "Epoch 186/500\n",
      "8s - loss: 1.7606 - act_output_loss: 0.7992 - time_output_loss: 0.9601 - val_loss: 1.7694 - val_act_output_loss: 0.8110 - val_time_output_loss: 0.9570\n",
      "Epoch 187/500\n",
      "8s - loss: 1.7481 - act_output_loss: 0.7924 - time_output_loss: 0.9543 - val_loss: 1.7709 - val_act_output_loss: 0.8129 - val_time_output_loss: 0.9567\n",
      "Epoch 188/500\n",
      "7s - loss: 1.7576 - act_output_loss: 0.8043 - time_output_loss: 0.9519 - val_loss: 1.7694 - val_act_output_loss: 0.8112 - val_time_output_loss: 0.9568\n",
      "Epoch 189/500\n",
      "8s - loss: 1.7560 - act_output_loss: 0.7976 - time_output_loss: 0.9571 - val_loss: 1.7696 - val_act_output_loss: 0.8118 - val_time_output_loss: 0.9565\n",
      "Epoch 190/500\n",
      "8s - loss: 1.7569 - act_output_loss: 0.7949 - time_output_loss: 0.9607 - val_loss: 1.7706 - val_act_output_loss: 0.8123 - val_time_output_loss: 0.9569\n",
      "Epoch 191/500\n",
      "7s - loss: 1.7561 - act_output_loss: 0.7939 - time_output_loss: 0.9608 - val_loss: 1.7706 - val_act_output_loss: 0.8125 - val_time_output_loss: 0.9568\n",
      "Epoch 192/500\n",
      "7s - loss: 1.7605 - act_output_loss: 0.8008 - time_output_loss: 0.9584 - val_loss: 1.7702 - val_act_output_loss: 0.8121 - val_time_output_loss: 0.9568\n",
      "Epoch 193/500\n",
      "7s - loss: 1.7603 - act_output_loss: 0.7992 - time_output_loss: 0.9597 - val_loss: 1.7703 - val_act_output_loss: 0.8119 - val_time_output_loss: 0.9571\n",
      "Epoch 194/500\n",
      "8s - loss: 1.7599 - act_output_loss: 0.7994 - time_output_loss: 0.9591 - val_loss: 1.7704 - val_act_output_loss: 0.8121 - val_time_output_loss: 0.9570\n",
      "Epoch 195/500\n",
      "8s - loss: 1.7473 - act_output_loss: 0.7918 - time_output_loss: 0.9542 - val_loss: 1.7703 - val_act_output_loss: 0.8120 - val_time_output_loss: 0.9569\n",
      "Epoch 196/500\n",
      "8s - loss: 1.7567 - act_output_loss: 0.7969 - time_output_loss: 0.9585 - val_loss: 1.7721 - val_act_output_loss: 0.8136 - val_time_output_loss: 0.9571\n",
      "Epoch 197/500\n",
      "7s - loss: 1.7576 - act_output_loss: 0.8004 - time_output_loss: 0.9558 - val_loss: 1.7700 - val_act_output_loss: 0.8118 - val_time_output_loss: 0.9569\n",
      "Epoch 198/500\n",
      "7s - loss: 1.7533 - act_output_loss: 0.7957 - time_output_loss: 0.9562 - val_loss: 1.7699 - val_act_output_loss: 0.8119 - val_time_output_loss: 0.9566\n",
      "Epoch 199/500\n",
      "7s - loss: 1.7630 - act_output_loss: 0.7998 - time_output_loss: 0.9618 - val_loss: 1.7686 - val_act_output_loss: 0.8104 - val_time_output_loss: 0.9568\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f52cc19bbe0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the model: \n",
    "print('Build model attention for activity and output...')\n",
    "main_input = Input(shape=(maxlen, num_features), name='main_input')\n",
    "\n",
    "# one shared layer\n",
    "l1 = LSTM(100, implementation=2, kernel_initializer=\"glorot_uniform\", dropout=0.2, return_sequences=True)(main_input)\n",
    "b1 = BatchNormalization()(l1)\n",
    "att_1 = AttentionWithContext()(b1)\n",
    "att_2 = AttentionWithContext(W_regularizer=regularizers.l2(0.2))(b1)\n",
    "# last activation\n",
    "\n",
    "act_output = Dense(len(targetchartoindice), activation=\"softmax\",\n",
    "                   kernel_initializer=\"glorot_uniform\", name=\"act_output\")(att_1)\n",
    "time_output = Dense(1, kernel_initializer=\"glorot_uniform\", name=\"time_output\")(att_2)\n",
    "\n",
    "\n",
    "\n",
    "model = Model(inputs=[main_input], outputs=[act_output, time_output])\n",
    "\n",
    "#compilations\n",
    "opt = optimizers.Nadam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004, clipvalue=3)\n",
    "model.compile(loss={'act_output':'categorical_crossentropy', 'time_output':'mean_absolute_error'}, optimizer=opt)\n",
    "\n",
    "#callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=83)\n",
    "model_checkpoint = ModelCheckpoint(args.outputdir + 'model_{epoch:02d}-{val_loss:.2f}.h5', \n",
    "                                   monitor='val_loss', verbose=0, save_best_only=True, \n",
    "                                   save_weights_only=False, mode='auto')\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, \n",
    "                               verbose=0, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "#fit\n",
    "model.fit(X, {'act_output':y_a, 'time_output':y_t}, validation_split=0.2, verbose=2, \n",
    "          callbacks=[early_stopping, model_checkpoint, lr_reducer], batch_size=16, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "main_input (InputLayer)          (None, 15, 14)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                    (None, 15, 100)       46000       main_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 15, 100)       400         lstm_3[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "attention_with_context_3 (Attent (None, 100)           10200       batch_normalization_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "attention_with_context_4 (Attent (None, 100)           10200       batch_normalization_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "act_output (Dense)               (None, 10)            1010        attention_with_context_3[0][0]   \n",
      "____________________________________________________________________________________________________\n",
      "time_output (Dense)              (None, 1)             101         attention_with_context_4[0][0]   \n",
      "====================================================================================================\n",
      "Total params: 67,911\n",
      "Trainable params: 67,711\n",
      "Non-trainable params: 200\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
